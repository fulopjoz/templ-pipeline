# TEMPL Pipeline Embedding Support - Deployment Guide

## Overview

This guide enables lightweight ESM2 embedding generation in the TEMPL Streamlit web app, allowing users to upload custom protein structures and generate embeddings on-demand.

## What Was Changed

### 1. Updated Requirements (`requirements-server.txt`)
- Added PyTorch CPU-only version for lightweight deployment
- Added Transformers library for ESM2 model support
- Added supporting dependencies (tokenizers, sentencepiece, protobuf)

### 2. Enhanced Dockerfile
- Optimized PyTorch installation with CPU index for smaller image size
- Added caching directories for Transformers models
- Maintained multi-stage build for efficiency
- Optional model pre-caching (commented out to reduce image size)

### 3. Fixed Embedding Generation (`embedding.py`)
- Fixed BFloat16 data type conversion issue
- Enhanced error handling and logging
- Maintained GPU/CPU compatibility

### 4. App Integration (`app.py`)
- Existing code already supports embedding features
- Automatic detection of PyTorch/Transformers availability
- User-friendly feedback when dependencies are missing

## Deployment Steps

### Option 1: Docker Deployment (Recommended)

```bash
# Build the updated container
docker build -t templ-pipeline:embedding-enabled .

# Run with embedding support
docker run -p 8080:8080 templ-pipeline:embedding-enabled
```

### Option 2: Local Development

```bash
# Install embedding dependencies
pip install torch --index-url https://download.pytorch.org/whl/cpu
pip install transformers tokenizers sentencepiece protobuf

# Run the app
python run_streamlit_app.py
```

## Features Enabled

### ‚úÖ What Works Now
- **Custom Protein Upload**: Users can upload PDB files and generate embeddings
- **ESM2 Integration**: Uses facebook/esm2_t33_650M_UR50D model
- **CPU Optimization**: Lightweight CPU-only deployment
- **Automatic Caching**: Generated embeddings are cached for reuse
- **Progress Feedback**: Real-time feedback during embedding generation
- **Fallback Support**: Graceful degradation when dependencies unavailable

### üîß Technical Details
- **Model**: ESM2 (650M parameters, ~2.6GB)
- **Device**: CPU-optimized (GPU support available if CUDA present)
- **Memory**: ~4-6GB RAM recommended for embedding generation
- **Performance**: 1-2 minutes per protein on CPU

## Verification

Run the test script to verify everything works:

```bash
python test_embedding_integration.py
```

Expected output:
```
üéâ All tests passed! Embedding functionality is ready.
```

## Container Size Impact

- **Before**: ~500MB (minimal dependencies only)
- **After**: ~2.8GB (includes PyTorch + Transformers)
- **Model Cache**: +2.6GB if pre-cached (optional)

## Performance Characteristics

### Embedding Generation Times (CPU)
- Small protein (100 residues): ~30 seconds
- Medium protein (300 residues): ~1 minute  
- Large protein (500+ residues): ~2 minutes

### Memory Usage
- Base app: ~500MB
- During embedding generation: ~2-4GB
- With cached model: +2.6GB

## User Experience

### Before (Limited)
- ‚ùå Could only use pre-computed embeddings
- ‚ùå No support for custom proteins
- ‚ùå Limited to database PDB IDs

### After (Full Featured)
- ‚úÖ Upload any PDB file
- ‚úÖ Generate embeddings on-demand
- ‚úÖ Real-time progress feedback
- ‚úÖ Automatic caching
- ‚úÖ Template similarity search

## Troubleshooting

### Common Issues

1. **Out of Memory**
   - Reduce container memory limits
   - Use smaller proteins for testing
   - Clear embedding cache if needed

2. **Slow Performance**
   - Expected on CPU (1-2 minutes per protein)
   - Consider GPU deployment for faster generation
   - Model downloads on first use

3. **Dependencies Missing**
   - Verify requirements-server.txt includes all dependencies
   - Check Docker build logs for installation errors

### Debug Commands

```bash
# Test embedding functionality
python test_embedding_integration.py

# Check app detection
python -c "from templ_pipeline.ui.app import EMBEDDING_FEATURES_AVAILABLE; print(f'Embeddings available: {EMBEDDING_FEATURES_AVAILABLE}')"

# Clear embedding cache
python -c "from templ_pipeline.core.embedding import EmbeddingManager; EmbeddingManager().clear_cache()"
```

## Next Steps

1. **Deploy**: Use the updated Dockerfile for production deployment
2. **Monitor**: Check memory usage and performance metrics
3. **Optimize**: Consider GPU deployment for better performance
4. **Scale**: Implement batch processing for multiple proteins

## Files Modified

- `requirements-server.txt` - Added embedding dependencies
- `Dockerfile` - Enhanced for embedding support
- `templ_pipeline/core/embedding.py` - Fixed BFloat16 conversion
- `test_embedding_integration.py` - Verification script (new)

The web app now supports full embedding generation capabilities while maintaining a reasonable deployment footprint for production use.
