[pytest]
# Performance testing configuration for pytest
# Optimized for performance benchmarking and monitoring

testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*

markers =
    fast: Fast tests (< 10s)
    medium: Medium tests (< 30s)
    slow: Slow tests (> 30s)
    integration: Integration tests
    performance: Performance tests
    ui: UI/Streamlit tests
    flaky: Flaky tests that may need retries
    critical: Critical path tests requiring high coverage
    isolation: Tests requiring isolation verification

# Performance testing with detailed profiling and timing
addopts = -v --tb=short --durations=0 --timeout=1800 --benchmark-only --benchmark-autosave --benchmark-compare-fail=mean:10% --benchmark-columns=mean,stddev,median,ops,rounds
# Performance-specific settings
asyncio_mode = auto
# Extended timeout for performance tests
timeout = 1800
timeout_method = thread

# Minimal warnings for cleaner performance output
filterwarnings =
    ignore::DeprecationWarning
    ignore::PendingDeprecationWarning
    ignore::pytest.PytestUnknownMarkWarning
    ignore::pytest.PytestCollectionWarning
    ignore::pytest.PytestDeprecationWarning

# Performance monitoring settings
benchmark_min_time = 0.000005
benchmark_max_time = 1.0
benchmark_min_rounds = 5
benchmark_timer = time.perf_counter